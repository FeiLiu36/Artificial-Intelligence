基础知识



the **expectation** of $f(x)$ , denoted by $\mathbb{E}[f]$ : the average value of some function $f(x)$ under a probability distribution $p(x)$.

for a discrete distribution
$$
\mathbb{E}[f]=\sum_{x} p(x) f(x)
$$
for a continuous distribution
$$
\mathbb{E}[f]=\int p(x) f(x) \mathrm{d} x
$$

---

样本 sample

总体 population

估计 estimation

期望 expectation

均值 mean

方差 variance/deviation

---

we have the **population mean** $\mu$ , **population variance** $\sigma^2$ 

i.e  $E[X]=\mu$                   $D[X]=E[(x-E[X])]^2=\sigma^2$

now we have the sample $x_1,x_2,\dots,x_n$

sample mean $\overline{x}=\frac{1}{n}\sum_{i=1}^{n}x_i$  ; sample variance $S^2=\frac{1}{n}\sum_{i=1}^{n}(x_i-\overline{x})^2$

we want to see the bias between sample mean/variance and population mean/variance 

we can see the sample mean expectation:

$$
E[\hat{\mu}]=E[\frac{1}{n}\sum_{i=1}^{n}x_i]=\frac{1}{n}\sum_{i=1}^{n}E[x_i]=\frac{1}{n}\sum_{i=1}^{n}\mu=\mu
$$

**it is unbiased expectation**

we can see the sample variance expectation:

$$
\begin{aligned}
E[\hat{\sigma^2}]=E[\frac{1}{n}\sum_{i=1}^{n}(x_i-\hat{\mu})^2]
&=\frac{1}{n}\sum_{i=1}^{n}E[(x_i-\hat{\mu})^2]\\
&=\frac{1}{n}\sum_{i=1}^{n}E(x_i^2-2x_i\hat{\mu} + \hat{\mu}^2)\\
&=\frac{1}{n}(\sum_{i=1}^{n}E[x_i^2]-2\sum_{i=1}^{n}E[x_i\hat{\mu}]+\sum_{i=1}^{n}E[\hat{\mu}^2])\\
&= 
\end{aligned}
$$



估计量的数学期望等于被估计参数的真实值，在多次重复下，它们的平均数接近所估计的参数真值 

Bias of an estimator :  is the difference between this estimator's expected value and the true value of the parameter being estimated. 



## Distribution

### 1.Gaussian

 $x \sim \mathcal{N}(\mu, \sigma)$
$$
p(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
$$
$E[x]=\mu, D[x]=\sigma^2$



The multivariate Gaussian distribution 多元高斯分布

 $\boldsymbol{x},\boldsymbol{\mu}$ are d-dimensional,  $\boldsymbol{x} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$
$$
p(\boldsymbol{x})==\frac{1}{(2 \pi)^{d / 2}|\boldsymbol{\Sigma}|^{1 / 2}} \exp \left\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right\}
$$
$E[\boldsymbol{x}]=\boldsymbol{\mu}, D[\boldsymbol{x}]=\boldsymbol{\Sigma}$ 

$\frac{\partial p(\boldsymbol{x})}{\partial \boldsymbol{x}}=-p(\boldsymbol{x}) \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})$

### 2.Poisson

$$
p(x=k | \lambda)=\frac{1}{k !} e^{-\lambda} \lambda^{k}
$$

$E[X] = D[X] = \lambda$



### 3.Exponential

$$
p(x | \lambda)=\lambda e^{-\lambda x}
$$

$E[x]=\frac{1}{\lambda}, D[x]=\frac{1}{\lambda^2}$


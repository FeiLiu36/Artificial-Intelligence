# Information Theory

> all $\log$ is base 2

## 1. Definition

the **informational value** of an event $x$ with probability $p(x)$ is:
$$
\mathrm{I}(x) = -\log(p(x))
$$
the **entropy** $Η$ of a discrete random variable $X$ with possible values $\left\{x_{1}, \ldots, x_{n}\right\}$ is:
$$
H(X)=\mathrm{E}[\mathrm{I}(X)]=\mathrm{E}[-\log (\mathrm{P}(X))]
$$
the **relative entropy** (also called **Kullback–Leibler divergence**) of discrete probability distribution $p$ and $q$ defined on the same probability space $\mathcal{X}$ is:
$$
D_{\mathrm{KL}}(p \| q)=\sum_{x \in \mathcal{X}} p(x) \log \left(\frac{p(x)}{q(x)}\right)
$$
the **cross entropy** of discrete probability distribution $p$ and $q$ with the same support $\mathcal{X}$ is:
$$
H(p, q) = -\mathrm{E}_{p}[\log q] = -\sum_{x \in \mathcal{X}} p(x) \log q(x)
$$
the relationship between relative entropy and cross entropy is:
$$
H(p, q)=H(p)+D_{\mathrm{KL}}(p \| q)
$$



## 2. Example









## 3. Recommend

https://blog.csdn.net/tsyccnh/article/details/79163834

https://blog.csdn.net/rtygbwwwerr/article/details/50778098



## 4. 

交叉熵为什么越小越好

什么地方使用交叉熵，和MSE比较有什么好处呢


# 主成分分析

> Principal Component Analysis (PCA)
>



## 概述

> 这一部分解释**什么是主成分**以及**为什么要找主成分**



（[维基百科](https://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90)）降维算法之一，为了保留数据中对<u>方差</u>贡献最大的特征。它利用正交变换来对一系列可能相关的变量的观测值进行线性变换，从而投影为一系列线性不相关变量的值，这些不相关变量称为主成分。



补充介绍一下什么是信噪比

> 在信号处理中存在**原始信号**和**噪声**两种
>
> 1. 噪声产生：处理过程中设备自行产生的，与原始信号无关，没原始信号时，就存在这样的噪声
>
> 2. 噪声特性：无规律，不随原始信号的变化而变化
>
> 3. 我们的目的：希望尽可能的只有原始信号，而没有噪声
> 4. 引入信噪比：=原始信号/噪声，音频信号一般用功率，其他信号一般用方差
>
> 5. 结论：信噪比越大，说明混在原始信号里的**噪声影响越小**，相比较而言**原始信号**具有**较大**的方差，**噪声**具有**较小**的方差



回到数据，其主成分可以被看成原始信号，非主成分被看成噪声，但因为他们是参杂在一起的，所以要想办法把主成分找出来。

因为



一直在谈找方差最大，方差最大所体现的是在某个方向上数据的差异性最大，也就是这个方向上信噪比最大，



将 $n$ 维特征映射到 $k$ 维，这 $k$ 维是相互正交的，每一维就是一个轴方向，



把数据变换到一个新的坐标系/基（坐标轴正交）中去，第一个坐标轴上的方差最大，第二个坐标轴次之，依次类推



也就是找到 $M$ 维中前 $K$ 个重要维度。如果把每一维都看作是一个特征的话，就是要找到哪几个特征是最重要的，是最明显的。

原基坐标系下的数据分布不够分散，希望找到在某个新基坐标系的轴上方差最大；同时还希望新数据不同维度能表示不重复的信息，即希望各维度线性无关



我们在找主成分的时候，就是要找到原始信号，去除噪声；因此就是要找到在某一方向上，信号的方差最大，被认为是原始信号



1. 将坐标轴中心移到数据的中心，然后旋转坐标轴，使得数据在C1轴上的方差最大，即全部数据个体在该方向上的投影最为分散。C1成为第一主成分。

2. 找一个C2，使得C2与C1的协方差（相关系数）为0，以免与C1信息重叠，并且使数据在该方向的方差尽量最大。

3. 以此类推，找到第三主成分，第四主成分……第k个主成分

   

## 图解例子







## 理论推导

> 过程中统一省略了零均值化，和求和平均

现在有 $N$ 个原始数据 $\{x^{(1)}, x^{(2)}, \dots, x^{(N)}\}$，其中每一个都是 $M$ 维的向量，记作 $X$：
$$
X=\left(\begin{array}{llll}
x^{(1)}_1 & x^{(2)}_1 & \cdots & x^{(N)}_1 \\
x^{(1)}_2 & x^{(2)}_2 & \cdots & x^{(N)}_2 \\
& & \dots \\
x^{(1)}_M & x^{(2)}_M & \cdots & x^{(N)}_M
\end{array}\right)
$$

希望将它降成 $K$ 维新数据，记作 $Y$：
$$
Y=\left(\begin{array}{llll}
x^{\prime(1)}_1 & x^{\prime(2)}_1 & \cdots & x^{\prime(N)}_1 \\
x^{\prime(1)}_2 & x^{\prime(2)}_2 & \cdots & x^{\prime(N)}_2 \\
& & \dots \\
x^{\prime(1)}_K & x^{\prime(2)}_K & \cdots & x^{\prime(N)}_K
\end{array}\right)
$$
这个过程其实是一个基变换，记作 $A$：
$$
Y = A X
$$
根据前面的概述，$Y$ 需要满足两个条件：

- 各个维度的方差最大
- 不同维度之间协方差为0



如何能表示方差和协方差呢，**协方差矩阵**！对角线元素为各个维度的方差，其余为不同维度之间的协方差。



原始数据 $X$ 的协方差矩阵为：
$$
XX^{\mathsf{T}} = \left(\begin{array}{llll}
\sum_{i=1}^N {x^{(i)}_1}^2 & \sum_{i=1}^N {x^{(i)}_1}{x^{(i)}_2} & \cdots & \sum_{i=1}^N {x^{(i)}_1}{x^{(i)}_M} \\
\sum_{i=1}^N {x^{(i)}_2}{x^{(i)}_1} & \sum_{i=1}^N {x^{(i)}_2}^2 & \cdots & \sum_{i=1}^N {x^{(i)}_2}{x^{(i)}_M} \\
& & \dots \\
\sum_{i=1}^N {x^{(i)}_M}{x^{(i)}_1} & \sum_{i=1}^N {x^{(i)}_M}{x^{(i)}_2} & \cdots & \sum_{i=1}^N {x^{(i)}_M}^2
\end{array}\right)
$$
新数据 $Y$ 的协方差矩阵为：
$$
Y Y^{\mathsf{T}} = AX (AX)^{\mathsf{T}} = A (X X^{\mathsf{T}}) A^{\mathsf{T}}
$$
前面提到的两个条件现在需要由 $Y Y^{\mathsf{T}}$ 来满足，即希望得到形如：
$$
Y Y^{\mathsf{T}}=\left(\begin{array}{llll}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
& & \dots \\
0 & 0 & \cdots & \lambda_K
\end{array}\right)
$$
看到这不禁想到了**矩阵的特征分解**，可以参考[链接](https://crazyang.blog.csdn.net/article/details/100540556)，对 $X X^{\mathsf{T}}$ 进行分解，$Y Y^{\mathsf{T}}$ 是特征值构成的矩阵，$A$ 是特征向量构成的矩阵。

> 为了方便理解，做一步变换，这里需要用到---因为 $A$ 是正交矩阵，所以 $A^{-1} = A^{\mathsf{T}}$

$$
X X^{\mathsf{T}} = A^{-1} (Y Y^{\mathsf{T}}) (A^{\mathsf{T}})^{-1} = A^{\mathsf{T}} (Y Y^{\mathsf{T}}) (A^{\mathsf{T}})^{-1}
$$

这就和标准的特征分解形式一致了，只是需要分解特征向量矩阵转置一下。



**反思：这个过程中我们好像没有去强调方差最大这个问题呀，这里面欠缺的逻辑其实是矩阵的特征向量和特征值是这个矩阵本身的固有性质，分解出来的特征值就是最大的，特征向量就是正交的。**



目前有两种主流的解释：

- 最大方差理论
- 最小化降维造成的损失

### 最大方差理论









## 算法实现

1. 每一维数据进行零均值化
2. 写出原数据的协方差矩阵
3. 求出协方差矩阵的特征值及对应的特征向量
4. 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 $K$ 行组成新基矩阵 $A$
5. $Y = A X$ 即为新数据



代码在matlab以及python中有现存包，使用方法如下：

```pythopn

```



## 如何确定主成分个数$K$

> 补充一个点

数据整体方差：$\frac{1}{N} \sum_{i=1}^N \|x^{i}\|^2$

数据映射方差：$\frac{1}{N} \sum_{i=1}^N \|x^{i} - x^{i}_{projection}\|^2$


$$
\frac{\frac{1}{N} \sum_{i=1}^N \|x^{i} - x^{i}_{projection}\|^2}{\frac{1}{N} \sum_{i=1}^N \|x^{i}\|^2} \le t
$$




$$
\min L = f(V(x)) + V(\mathbf{x})\\
$$





> **最后一句总结：主成分是数据本身的一个固有性质，不是为了找而找，而是它就在那里。**
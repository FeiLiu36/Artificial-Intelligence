**[`evolutionary-stochastic-gradient-descent-for-optimization-of-deep-neural-networks`]**

**[`2018`]** **[`NIPS`]** **[[:memo:]()]** **[[:octocat:]()]**

<details><summary>Click to expand</summary><p>


**Abstract:**

> They propose a population-based Evolutionary Stochastic Gradient Descent (ESGD)
> framework for optimizing deep neural networks.
>
> This ESGD framwork can be applied on  speech recognition, image recognition and language modeling, using networks with a variety of deep architectures.

**The methods it used:** 

- [ ] Several ways of attack: Fast Gradient Sign Method (FGSM), Randomized Fast Gradient Sign Method (RAND+FGSM), The Carlini-Wagner (CW) attack
- [ ] Lebesgue-measure

**Its contribution:**

> They proposed a novel defense strategy utilizing GANs to enhance the
> robustness of classification models against black-box and white-box adversarial attacks

**My Comments:**

> This work can be referred to using AE (Auto Encoder) for noise reduction. Itâ€™s just an easy application of GANs.
>

</p></details>

---
# GMM高斯混合模型

> Gaussian Mixed Model

 

## 模型构建

有 $n$ 个观测数据：$x_1,x_2,\dots,x_n$

有 $K$ 个同分布不同参数的模型：$k=1,2,\dots,K$ 

第 $k$ 个模型分量的概率密度：$\pi_k$，满足条件：$0 \leq \pi_{k} \leq 1$ , $\sum_{k=1}^{K} \pi_{k}=1$ 

第 $i$ 个观测数据属于第 $k$ 个模型分量的概率密度：$\gamma_{ik}$ 

一个 $K$ 维随机变量 $z$，采用“1-of-K”表示方法，其中一个分量 $z_k=1$ ，其余分量都等于0，表示第 $k$ 个模型被选中的概率，即 $p(z_k=1)=\pi_k$，满足条件：$z_{k} \in\{0,1\}$，$\sum_{k=1}^{K} z_{k}=1$ 。



> 其实每一个数据 $x_i$ 都对应一个 $z_i$ ，而 $z_i$ 是 $K$ 维的，所以完整的写法应该是 $z_{ik}$ ，最终构成的是是一个 $n×k$ 维的！！！



> 问，$z_1=1$ 和 $z_2=1$ 的意义？
>
> 有 $n$ 个 $\boldsymbol{z}$，完整来讲应该是
>
> 如： $z_{11}=1$ ，而 $z_{12},z_{13},\dots,z_{1K}$ 全为0
>
> 又如： $z_{23}=1$ ，而 $z_{21},z_{22},\dots,z_{2K}$ 全为0
>
> 而这 $n$ 个 $\boldsymbol{z}$ 中是有相同的存在，共有 $K$ 种不同的状态



 $\boldsymbol{x}$ 和 $\boldsymbol{z}$ 是一个向量随机变量，每一个观测数据 $\boldsymbol{x}_i$ 都对应一个 隐变量 $\boldsymbol{z}_i$ ，很多时候将 $z_{ik}$ 写成 $z_k$ ，专指了与之对应的 $\boldsymbol{x}_i$  

为了避免混淆，后面我将全部使用完全态来表示
$$
p(z_{ik}=1)=\pi_k
$$

$$
p(z_i)=\prod_{k=1}^{K} \pi_{k}^{z_ik}
$$


$$
p\left(\boldsymbol{x} | z_{k}=1\right)=\mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{k}, \mathbf{\Sigma}_{k}\right)
$$

$$
p(\boldsymbol{x} | \boldsymbol{z})=\prod_{k=1}^{K} \mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)^{z_k}
$$
联合概率密度 
$$
p(\boldsymbol{x},\boldsymbol{z})=p(\boldsymbol{x}|\boldsymbol{z})p(\boldsymbol{z})=\prod_{k=1}^{K} \left[\mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\pi_k \right]^{z_k}
$$

$$
p(\boldsymbol{x},z_k=1)=\pi_k\mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)
$$

边缘概率密度
$$
p(\boldsymbol{x})=\sum_{k=1}^{K}p(\boldsymbol{x},z_k=1)=\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{k}, \mathbf{\Sigma}_{k}\right)
$$


因为z有K种不同的值（情况），得到了混合高斯概率分布函数：
$$
p(\boldsymbol{x})=\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{k}, \mathbf{\Sigma}_{k}\right)
$$



给定x，想要知道z 的条件概率
$$
\begin{aligned} \gamma\left(z_{k}\right) \equiv p\left(z_{k}=1 | \boldsymbol{x}\right) &=\frac{p\left(z_{k}=1\right) p\left(\boldsymbol{x} | z_{k}=1\right)}{\sum_{j=1}^{K} p\left(z_{j}=1\right) p\left(\boldsymbol{x} | z_{j}=1\right)} \\ &=\frac{\pi_{k} \mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(\boldsymbol{x} | \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{k}\right)} \end{aligned}
$$



---

最大似然

根据公式8，对数似然函数为：
$$
\ln p(\boldsymbol{X} | \boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{\Sigma})=\sum_{n=1}^{N} \ln \left\{\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right\}
$$

令公式9中的$\ln p(\boldsymbol{X} | \boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{\Sigma})$ 关于高斯分量的均值 $\boldsymbol\mu_k$ 的均值等于0，偏导数为0
$$
0=\sum_{n=1}^{K} \frac{\pi_{k} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{k}, \mathbf{\Sigma}_{k}\right)}{\underbrace{\sum_{j} \pi_{j} \mathcal{N}\left(\boldsymbol{x}_{n} | \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)}}_{\gamma\left(z_{n k}\right)} \boldsymbol{\Sigma}_{k}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{k}\right)
$$







https://blog.csdn.net/jinping_shi/article/details/59613054

https://blog.csdn.net/v_july_v/article/details/81708386

https://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html

http://www.csuldw.com/2015/12/02/2015-12-02-EM-algorithms/

https://zhuanlan.zhihu.com/p/30483076



https://github.com/PRML/PRMLT

https://github.com/ctgk/PRML
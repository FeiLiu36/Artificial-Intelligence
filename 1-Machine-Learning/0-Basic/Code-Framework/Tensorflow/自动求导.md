讲解一下 `tf.GradientTape()`



```python
x = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
dy_dx = g.gradient(y, x) # Will compute to 6.0
```



调用 `GradientTape.gradient()` 方法时，其占用的资源就会立即释放，不能再次调用。如果想要不被释放，使用 `tf.GradientTape(persistent=True)` ，这样还需要在使用完后 `del`

```python
x = tf.constant(3.0)
with tf.GradientTape(persistent=True) as t:
  t.watch(x)
  y = x * x
  z = y * y
dz_dx = t.gradient(z, x)  # 108.0 (4*x^3 at x = 3)
dy_dx = t.gradient(y, x)  # 6.0
del t  # Drop the reference to the tape
```

